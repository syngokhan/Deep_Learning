{"cells":[{"cell_type":"markdown","metadata":{"id":"NDw_jQC3f1za"},"source":["# Stage 1: Importing dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWJXHHxyfvUA"},"outputs":[],"source":["import numpy as np\n","import math\n","import re\n","import time\n","from google.colab import drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uv_nm_AugDsV"},"outputs":[],"source":["try:\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","import tensorflow_datasets as tfds"]},{"cell_type":"markdown","metadata":{"id":"kc7Wf_yngJGw"},"source":["# Stage 2: Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"2Cv1Pb3xgLkL"},"source":["## Loading files"]},{"cell_type":"markdown","metadata":{"id":"Y-X9aj9IgPbv"},"source":["We import files from our personal google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21865,"status":"ok","timestamp":1659345198086,"user":{"displayName":"gokhan ersoz","userId":"13455147674345597309"},"user_tz":-180},"id":"QkjG8jiggLHo","outputId":"a981b1ab-eb33-4232-ffbc-633fe1c043e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EAxdPLy2n75"},"outputs":[],"source":["en = \"/content/drive/MyDrive/Deep_Learning/BERT&Transformer/Model_NLP/Data/nonbreaking_prefix.en\"\n","fr = \"/content/drive/MyDrive/Deep_Learning/BERT&Transformer/Model_NLP/Data/nonbreaking_prefix.fr\"\n","europarl_en_path = \"/content/drive/MyDrive/Deep_Learning/BERT&Transformer/Model_NLP/Data/fr-en/europarl-v7.fr-en.en\"\n","europarl_fr_path = \"/content/drive/MyDrive/Deep_Learning/BERT&Transformer/Model_NLP/Data/fr-en/europarl-v7.fr-en.fr\"\n","\n","with open(\"/content/drive/MyDrive/Deep_Learning/BERT&Transformer/Model_NLP/Data/fr-en/europarl-v7.fr-en.en\",\n","          mode='r',\n","          encoding=\"utf-8\") as f:\n","    europarl_en = f.read()\n","with open(\"/content/drive/MyDrive/Deep_Learning/BERT&Transformer/Model_NLP/Data/fr-en/europarl-v7.fr-en.fr\",\n","          mode='r',\n","          encoding=\"utf-8\") as f:\n","    europarl_fr = f.read()\n","with open(\"/content/drive/MyDrive/Deep_Learning/BERT&Transformer/Model_NLP/Data/nonbreaking_prefix.en\",\n","          mode='r',\n","          encoding=\"utf-8\") as f:\n","    non_breaking_prefix_en = f.read()\n","with open(\"/content/drive/MyDrive/Deep_Learning/BERT&Transformer/Model_NLP/Data/nonbreaking_prefix.fr\",\n","          mode='r',\n","          encoding=\"utf-8\") as f:\n","    non_breaking_prefix_fr = f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":291,"status":"ok","timestamp":1659345228046,"user":{"displayName":"gokhan ersoz","userId":"13455147674345597309"},"user_tz":-180},"id":"0fNU_Mbyg-S4","outputId":"2124f0f9-8fb3-4501-ec30-9086d46473c8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Resumption of the session\\nI declare resumed the se'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["europarl_en[:50]"]},{"cell_type":"code","source":["europarl_fr[:50]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"YG6hdcCIrtgj","executionInfo":{"status":"ok","timestamp":1659345238234,"user_tz":-180,"elapsed":270,"user":{"displayName":"gokhan ersoz","userId":"13455147674345597309"}},"outputId":"fdc57e55-cfe5-4594-a3d0-1dec371d4a24"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Reprise de la session\\nJe dÃ©clare reprise la sessio'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"ZzAviIErhKEH"},"source":["## Cleaning data"]},{"cell_type":"markdown","metadata":{"id":"VoBBMXpUhM8U"},"source":["Getting the non_breaking_prefixes as a clean list of words with a point at the end so it is easier to use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NBaz0Y-uhLQA"},"outputs":[],"source":["non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n","non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n","\n","non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n","non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"]},{"cell_type":"markdown","metadata":{"id":"zRy8a6WYhl7Z"},"source":["We will need each word and other symbol that we want to keep to be in lower case and separated by spaces so we can \"tokenize\" them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-F5KlmMhttk"},"outputs":[],"source":["corpus_en = europarl_en\n","for prefix in non_breaking_prefix_en:\n","    corpus_en = corpus_en.replace(prefix, prefix + \"###\")\n","corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_en)\n","corpus_en = re.sub(r\"\\.###\", '', corpus_en)\n","corpus_en = re.sub(r\"  +\", ' ', corpus_en)\n","corpus_en = corpus_en.split(\"\\n\")\n","\n","corpus_fr = europarl_fr\n","for prefix in non_breaking_prefix_fr:\n","    corpus_fr = corpus_fr.replace(prefix, prefix + \"###\")\n","corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_fr)\n","corpus_fr = re.sub(r\"\\.###\", '', corpus_fr)\n","corpus_fr = re.sub(r\"  +\", ' ', corpus_fr)\n","corpus_fr = corpus_fr.split(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"l0gJoNgL02yY"},"source":["## Tokenizing text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_nBLPM00Rhy"},"outputs":[],"source":["tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    corpus_en, target_vocab_size=2**13)\n","tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    corpus_fr, target_vocab_size=2**13)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1659345995260,"user":{"displayName":"gokhan ersoz","userId":"13455147674345597309"},"user_tz":-180},"id":"8ktLO0eD1O20","outputId":"98a43d17-c956-4187-df5f-cab741b0aeb5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8190, 8171)"]},"metadata":{},"execution_count":11}],"source":["VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n","VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2\n","VOCAB_SIZE_EN,VOCAB_SIZE_FR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251043,"status":"ok","timestamp":1659346246299,"user":{"displayName":"gokhan ersoz","userId":"13455147674345597309"},"user_tz":-180},"id":"VVqHegbp1XnK","outputId":"b1904691-53e3-4c21-8d96-60bf8c3bb76b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[8188, 4399, 962, 2124, 3, 1, 2528, 8189]"]},"metadata":{},"execution_count":12}],"source":["inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n","          for sentence in corpus_en]\n","outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n","          for sentence in corpus_fr]"]},{"cell_type":"code","source":["print(\"Vocab Size En : \" , VOCAB_SIZE_EN)\n","print(\"Inputs Size : \", len(inputs))\n","print()\n","print(\"Vocab Size Fr : \", VOCAB_SIZE_FR)\n","print(\"Outputs Size : \", len(outputs))\n","print()"],"metadata":{"id":"rXfZib67C4B8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs[0],outputs[0]"],"metadata":{"id":"rieUbbr-C07o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nuaPXaDg1qDQ"},"source":["## Remove too long sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"efQb8K6O1rfc"},"outputs":[],"source":["MAX_LENGTH = 20\n","idx_to_remove = [count for count, sent in enumerate(inputs)\n","                 if len(sent) > MAX_LENGTH]\n","for idx in reversed(idx_to_remove):\n","    del inputs[idx]\n","    del outputs[idx]\n","idx_to_remove = [count for count, sent in enumerate(outputs)\n","                 if len(sent) > MAX_LENGTH]\n","for idx in reversed(idx_to_remove):\n","    del inputs[idx]\n","    del outputs[idx]"]},{"cell_type":"code","source":["print(\"Before En, Fr Length : \", len(europarl_en),\"---\",len(europarl_fr))\n","print()\n","print(\"After En, Fr Length : \",len(inputs),\"---\",len(outputs))"],"metadata":{"id":"BogU9uz8Cw2y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dUxUThRH2HFf"},"source":["## Inputs/outputs creation"]},{"cell_type":"markdown","metadata":{"id":"xvPC9nPN2M6B"},"source":["As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOE-P2lT2MQp"},"outputs":[],"source":["inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n","                                                       value=0,\n","                                                       padding=\"post\",\n","                                                       maxlen=MAX_LENGTH)\n","outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n","                                                        value=0,\n","                                                        padding=\"post\",\n","                                                        maxlen=MAX_LENGTH)"]},{"cell_type":"code","source":["inputs[0],outputs[0]"],"metadata":{"id":"eZnis7lVCsgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uINRFhOm2mu1"},"outputs":[],"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 20000\n","\n","dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n","\n","dataset = dataset.cache()\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"Wn2w4DqH288u"},"source":["# Stage 3: Model building"]},{"cell_type":"markdown","metadata":{"id":"BcFBxDRZ2_WT"},"source":["## Embedding"]},{"cell_type":"markdown","metadata":{"id":"v0aOFAzp3Buh"},"source":["Positional encoding formulae:\n","\n","$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/dmodel})$\n","\n","$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/dmodel})$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dr0S1uBX2-_i"},"outputs":[],"source":["class PositionalEncoding(layers.Layer):\n","\n","    def __init__(self):\n","        super(PositionalEncoding, self).__init__()\n","    \n","    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n","        angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n","        return pos * angles # (seq_length, d_model)\n","    \n","    def call(self, inputs):\n","        seq_length = inputs.shape.as_list()[-2]\n","        d_model = inputs.shape.as_list()[-1]\n","        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n","                                 np.arange(d_model)[np.newaxis, :],\n","                                 d_model)\n","        angles[:, 0::2] = np.sin(angles[:, 0::2])\n","        angles[:, 1::2] = np.cos(angles[:, 1::2])\n","        pos_encoding = angles[np.newaxis, ...]\n","\n","        return inputs + tf.cast(pos_encoding, tf.float32)"]},{"cell_type":"markdown","metadata":{"id":"uCkTpS5X4oSw"},"source":["## Attention"]},{"cell_type":"markdown","metadata":{"id":"LOXSznTT4r0C"},"source":["### Attention computation"]},{"cell_type":"markdown","metadata":{"id":"VilQe5gT4uA3"},"source":["$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LO49thja5EaF"},"outputs":[],"source":["def scaled_dot_product_attention(queries, keys, values, mask):\n","    product = tf.matmul(queries, keys, transpose_b=True)\n","\n","    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n","    scaled_product = product / tf.math.sqrt(keys_dim)\n","\n","    if mask is not None:\n","        scaled_product += (mask * -1e9)\n","    \n","    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n","\n","    return attention"]},{"cell_type":"markdown","metadata":{"id":"eYhP6g5i5cba"},"source":["### Multi-head attention sublayer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsK2rhm95eRx"},"outputs":[],"source":["class MultiHeadAttention(layers.Layer):\n","\n","    def __init__(self, nb_proj):\n","        super(MultiHeadAttention, self).__init__()\n","        self.nb_proj = nb_proj\n","    \n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","        assert self.d_model % self.nb_proj == 0\n","\n","        self.d_proj = self.d_model // self.nb_proj\n","\n","        self.query_lin = layers.Dense(units=self.d_model)\n","        self.key_lin = layers.Dense(units=self.d_model)\n","        self.value_lin = layers.Dense(units=self.d_model)\n","\n","        self.final_lin = layers.Dense(units=self.d_model)\n","    \n","    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n","        shape = (batch_size,\n","                 -1,\n","                 self.nb_proj,\n","                 self.d_proj)\n","        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n","        \n","        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n","    \n","    def call(self, queries, keys, values, mask):\n","        batch_size = tf.shape(queries)[0]\n","\n","        queries = self.query_lin(queries)\n","        keys = self.key_lin(keys)\n","        values = self.value_lin(values)\n","\n","        queries = self.split_proj(queries, batch_size)\n","        keys = self.split_proj(keys, batch_size)\n","        values = self.split_proj(values, batch_size)\n","\n","        attention = scaled_dot_product_attention(queries, keys, values, mask)\n","\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n","\n","        concat_attention = tf.reshape(attention,\n","                                      shape=(batch_size, -1, self.d_model))\n","        \n","        outputs = self.final_lin(concat_attention)\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"a88reCtV7Vo-"},"source":["## Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_wCHJAQ7XZu"},"outputs":[],"source":["class EncoderLayer(layers.Layer):\n","\n","    def __init__(self, FFN_units, nb_proj, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.FFN_units = FFN_units\n","        self.nb_proj = nb_proj\n","        self.dropout = dropout\n","    \n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","\n","        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n","        self.dropout_1 = layers.Dropout(rate=self.dropout)\n","        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n","        self.dense_2 = layers.Dense(units=self.d_model)\n","        self.dropout_2 = layers.Dropout(rate=self.dropout)\n","        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n","    \n","    def call(self, inputs, mask, training):\n","        attention = self.multi_head_attention(inputs,\n","                                              inputs,\n","                                              inputs,\n","                                              mask)\n","        attention = self.dropout_1(attention, training=training)\n","        attention = self.norm_1(attention + inputs)\n","\n","        outputs = self.dense_1(attention)\n","        outputs = self.dense_2(outputs)\n","        outputs = self.dropout_2(outputs, training=training)\n","        outputs = self.norm_2(outputs + attention)\n","\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6QNvI518mpG"},"outputs":[],"source":["class Encoder(layers.Layer):\n","\n","    def __init__(self,\n","                 nb_layers,\n","                 FFN_units,\n","                 nb_proj,\n","                 dropout,\n","                 vocab_size,\n","                 d_model,\n","                 name=\"encoder\"):\n","        super(Encoder, self).__init__(name=name)\n","        self.nb_layers = nb_layers\n","        self.d_model = d_model\n","\n","        self.embedding = layers.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding()\n","        self.dropout = layers.Dropout(rate=dropout)\n","        self.enc_layers = [EncoderLayer(FFN_units,\n","                                        nb_proj,\n","                                        dropout)\n","                           for _ in range(nb_layers)]\n","    \n","    def call(self, inputs, mask, training):\n","        outputs = self.embedding(inputs)\n","        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        outputs = self.pos_encoding(outputs)\n","        outputs = self.dropout(outputs, training)\n","\n","        for i in range(self.nb_layers):\n","            outputs = self.enc_layers[i](outputs, mask, training)\n","        \n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"OFCrh2G_-mKx"},"source":["## Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bofPBDZ0-nQ7"},"outputs":[],"source":["class DecoderLayer(layers.Layer):\n","\n","    def __init__(self, FFN_units, nb_proj, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.FFN_units = FFN_units\n","        self.nb_proj = nb_proj\n","        self.dropout = dropout\n","    \n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","\n","        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n","        self.dropout_1 = layers.Dropout(rate=self.dropout)\n","        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n","        self.dropout_2 = layers.Dropout(rate=self.dropout)\n","        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n","        self.dense_2 = layers.Dense(units=self.d_model)\n","        self.dropout_3 = layers.Dropout(rate=self.dropout)\n","        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n","    \n","    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n","        attention = self.multi_head_attention_1(inputs,\n","                                                inputs,\n","                                                inputs,\n","                                                mask_1)\n","        attention = self.dropout_1(attention, training=training)\n","        attention = self.norm_1(attention + inputs)\n","\n","        attention_2 = self.multi_head_attention_2(attention,\n","                                                  enc_outputs,\n","                                                  enc_outputs,\n","                                                  mask_2)\n","        attention_2 = self.dropout_2(attention_2, training=training)\n","        attention_2 = self.norm_2(attention_2 + attention)\n","\n","        outputs = self.dense_1(attention_2)\n","        outputs = self.dense_2(outputs)\n","        outputs = self.dropout_3(outputs, training=training)\n","        outputs = self.norm_3(outputs + attention_2)\n","\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3hoLUqE_1HJ"},"outputs":[],"source":["class Decoder(layers.Layer):\n","\n","    def __init__(self,\n","                 nb_layers,\n","                 FFN_units,\n","                 nb_proj,\n","                 dropout,\n","                 vocab_size,\n","                 d_model,\n","                 name=\"decoder\"):\n","        super(Decoder, self).__init__(name=name)\n","        self.nb_layers = nb_layers\n","        self.d_model = d_model\n","\n","        self.embedding = layers.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding()\n","        self.dropout = layers.Dropout(rate=dropout)\n","        self.dec_layers = [DecoderLayer(FFN_units,\n","                                        nb_proj,\n","                                        dropout)\n","                           for _ in range(nb_layers)]\n","    \n","    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n","        outputs = self.embedding(inputs)\n","        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        outputs = self.pos_encoding(outputs)\n","        outputs = self.dropout(outputs, training)\n","\n","        for i in range(self.nb_layers):\n","            outputs = self.dec_layers[i](outputs,\n","                                         enc_outputs,\n","                                         mask_1,\n","                                         mask_2,\n","                                         training)\n","        \n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"pf_YD4anAbfb"},"source":["## Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTmNakXjAc7U"},"outputs":[],"source":["class Transformer(tf.keras.Model):\n","\n","    def __init__(self,\n","                 vocab_size_enc,\n","                 vocab_size_dec,\n","                 d_model,\n","                 nb_layers,\n","                 FFN_units,\n","                 nb_proj,\n","                 dropout,\n","                 name=\"transformer\"):\n","        super(Transformer, self).__init__(name=name)\n","\n","        self.encoder = Encoder(nb_layers,\n","                               FFN_units,\n","                               nb_proj,\n","                               dropout,\n","                               vocab_size_enc,\n","                               d_model)\n","        \n","        self.decoder = Decoder(nb_layers,\n","                               FFN_units,\n","                               nb_proj,\n","                               dropout,\n","                               vocab_size_dec,\n","                               d_model)\n","        \n","        self.last_linear = layers.Dense(units=vocab_size_dec)\n","    \n","    def create_padding_mask(self, seq): # seq: (batch_size, seq_length)\n","        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","        return mask[:, tf.newaxis, tf.newaxis, :]\n","\n","    def create_look_ahead_mask(self, seq):\n","        seq_len = tf.shape(seq)[1]\n","        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","        return look_ahead_mask\n","    \n","    def call(self, enc_inputs, dec_inputs, training):\n","        enc_mask = self.create_padding_mask(enc_inputs)\n","        dec_mask_1 = tf.maximum(\n","            self.create_padding_mask(dec_inputs),\n","            self.create_look_ahead_mask(dec_inputs)\n","        )\n","        dec_mask_2 = self.create_padding_mask(enc_inputs)\n","\n","        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n","        dec_outputs = self.decoder(dec_inputs,\n","                                   enc_outputs,\n","                                   dec_mask_1,\n","                                   dec_mask_2,\n","                                   training)\n","        \n","        outputs = self.last_linear(dec_outputs)\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"mIBSpVE_B9NA"},"source":["# Stage 4: Application"]},{"cell_type":"markdown","metadata":{"id":"5JYN0U3NCChb"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycc6XrOfCAgN"},"outputs":[],"source":["tf.keras.backend.clear_session()\n","\n","# Hyper-parameters\n","D_MODEL = 128 # 512\n","NB_LAYERS = 4 # 6\n","FFN_UNITS = 512 # 2048\n","NB_PROJ = 8 # 8\n","DROPOUT = 0.1 # 0.1\n","\n","transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n","                          vocab_size_dec=VOCAB_SIZE_FR,\n","                          d_model=D_MODEL,\n","                          nb_layers=NB_LAYERS,\n","                          FFN_units=FFN_UNITS,\n","                          nb_proj=NB_PROJ,\n","                          dropout=DROPOUT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYW49iYZCjds"},"outputs":[],"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n","                                                            reduction=\"none\")\n","def loss_function(target, pred):\n","    mask = tf.math.logical_not(tf.math.equal(target, 0))\n","    loss_ = loss_object(target, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)\n","\n","train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzRVfJW_DGHG"},"outputs":[],"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = tf.cast(d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","    \n","learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate,\n","                                     beta_1=0.9,\n","                                     beta_2=0.98,\n","                                     epsilon=1e-9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbxJnPFNEUgt"},"outputs":[],"source":["checkpoint_path = \"./drive/MyDrive/projects/transformer/ckpt/\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer,\n","                           optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print(\"Latest checkpoint restored!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7q2LuvbE8xM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e455c5c0-de96-47b2-87c5-d75017bfc747"},"outputs":[{"output_type":"stream","name":"stdout","text":["Start of epoch 1\n","Epoch 1 Batch 0 Loss 6.7052 Accuracy 0.0008\n","Epoch 1 Batch 50 Loss 6.2453 Accuracy 0.0134\n","Epoch 1 Batch 100 Loss 6.2432 Accuracy 0.0317\n","Epoch 1 Batch 150 Loss 6.1619 Accuracy 0.0386\n","Epoch 1 Batch 200 Loss 6.0526 Accuracy 0.0421\n","Epoch 1 Batch 250 Loss 5.9402 Accuracy 0.0446\n","Epoch 1 Batch 300 Loss 5.8097 Accuracy 0.0511\n","Epoch 1 Batch 350 Loss 5.6877 Accuracy 0.0571\n","Epoch 1 Batch 400 Loss 5.5496 Accuracy 0.0616\n","Epoch 1 Batch 450 Loss 5.4301 Accuracy 0.0654\n","Epoch 1 Batch 500 Loss 5.3183 Accuracy 0.0700\n","Epoch 1 Batch 550 Loss 5.2171 Accuracy 0.0754\n","Epoch 1 Batch 600 Loss 5.1184 Accuracy 0.0808\n","Epoch 1 Batch 650 Loss 5.0275 Accuracy 0.0863\n","Epoch 1 Batch 700 Loss 4.9406 Accuracy 0.0916\n","Epoch 1 Batch 750 Loss 4.8548 Accuracy 0.0968\n","Epoch 1 Batch 800 Loss 4.7732 Accuracy 0.1019\n","Epoch 1 Batch 850 Loss 4.6944 Accuracy 0.1070\n","Epoch 1 Batch 900 Loss 4.6201 Accuracy 0.1118\n","Epoch 1 Batch 950 Loss 4.5503 Accuracy 0.1164\n","Epoch 1 Batch 1000 Loss 4.4834 Accuracy 0.1208\n","Epoch 1 Batch 1050 Loss 4.4222 Accuracy 0.1249\n","Epoch 1 Batch 1100 Loss 4.3664 Accuracy 0.1288\n","Epoch 1 Batch 1150 Loss 4.3124 Accuracy 0.1322\n","Epoch 1 Batch 1200 Loss 4.2621 Accuracy 0.1355\n","Epoch 1 Batch 1250 Loss 4.2137 Accuracy 0.1388\n","Epoch 1 Batch 1300 Loss 4.1678 Accuracy 0.1419\n","Epoch 1 Batch 1350 Loss 4.1245 Accuracy 0.1451\n","Epoch 1 Batch 1400 Loss 4.0835 Accuracy 0.1480\n","Epoch 1 Batch 1450 Loss 4.0434 Accuracy 0.1510\n","Epoch 1 Batch 1500 Loss 4.0043 Accuracy 0.1541\n","Epoch 1 Batch 1550 Loss 3.9680 Accuracy 0.1570\n"]}],"source":["EPOCHS = 10\n","for epoch in range(EPOCHS):\n","    print(\"Start of epoch {}\".format(epoch+1))\n","    start = time.time()\n","\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","\n","    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n","        dec_inputs = targets[:, :-1]\n","        dec_outputs_real = targets[:, 1:]\n","        with tf.GradientTape() as tape:\n","            predictions = transformer(enc_inputs, dec_inputs, True)\n","            loss = loss_function(dec_outputs_real, predictions)\n","        \n","        gradients = tape.gradient(loss, transformer.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","        train_loss(loss)\n","        train_accuracy(dec_outputs_real, predictions)\n","\n","        if batch % 50 == 0:\n","            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n","                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n","    \n","    ckpt_save_path = ckpt_manager.save()\n","    print(\"Saving checkpont for epoch {} at {}\".format(epoch+1, ckpt_save_path))\n","    print(\"time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"]},{"cell_type":"markdown","metadata":{"id":"L2vY9ogwzFW0"},"source":["##Â Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umPNHaoRGTxj"},"outputs":[],"source":["def evaluate(inp_sentence):\n","    inp_sentence = \\\n","        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n","    enc_input = tf.expand_dims(inp_sentence, axis=0)\n","\n","    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n","\n","    for _ in range(MAX_LENGTH):\n","        predictions = transformer(enc_input, output, False) # (1, seq_length, vocab_size_fr)\n","\n","        prediction = predictions[:, -1:, :]\n","\n","        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n","\n","        if predicted_id == VOCAB_SIZE_FR-1:\n","            return tf.squeeze(output, axis=0)\n","        \n","        output = tf.concat([output, predicted_id], axis=-1)\n","    \n","    return tf.squeeze(output, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkYA-uifz2J4"},"outputs":[],"source":["def translate(sentence):\n","    output = evaluate(sentence).numpy()\n","\n","    predicted_sentence = tokenizer_fr.decode(\n","        [i for i in output if i < VOCAB_SIZE_FR-2]\n","    )\n","\n","    print(\"Input: {}\".format(sentence))\n","    print(\"Predicted translation: {}\".format(predicted_sentence))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQS6keta0IMw"},"outputs":[],"source":["translate(\"This is great.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CIGtLSe7VfPg"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Transformer_for_NLP.ipynb","provenance":[{"file_id":"1wJD_GAfLw62A0qE7Pkd59mT8C_4KDOrX","timestamp":1658257340251}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}