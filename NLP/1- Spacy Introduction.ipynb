{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "corresponding-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -U, --upgrade  \n",
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "utility-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy-lookups-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "useful-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -m mod : run library module as a script (terminates option list)\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-eclipse",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enclosed-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "talented-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "impaired-voluntary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on English in module spacy.lang.en object:\n",
      "\n",
      "class English(spacy.language.Language)\n",
      " |  English(vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |  \n",
      " |  A text-processing pipeline. Usually you'll load this once per process,\n",
      " |  and pass the instance around your application.\n",
      " |  \n",
      " |  Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
      " |      object and processing pipeline.\n",
      " |  lang (str): IETF language code, such as 'en'.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/language\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      English\n",
      " |      spacy.language.Language\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Defaults = <class 'spacy.lang.en.EnglishDefaults'>\n",
      " |      Language data defaults, available via Language.Defaults. Can be\n",
      " |      overwritten by language subclasses by defining their own subclasses of\n",
      " |      Language.Defaults.\n",
      " |  \n",
      " |  default_config = {'paths': {'train': None, 'dev': None, 'vectors'...s'...\n",
      " |  \n",
      " |  factories = {'attribute_ruler': <function make_attribute_rul...r': <fu...\n",
      " |  \n",
      " |  lang = 'en'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, spacy.tokens.doc.Doc], *, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None) -> spacy.tokens.doc.Doc\n",
      " |      Apply the pipeline to some text. The text can span multiple sentences,\n",
      " |      and can contain arbitrary whitespace. Alignment into the original string\n",
      " |      is preserved.\n",
      " |      \n",
      " |      text (Union[str, Doc]): If `str`, the text to be processed. If `Doc`,\n",
      " |          the doc will be passed directly to the pipeline, skipping\n",
      " |          `Language.make_doc`.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, dict]): An optional dictionary with extra\n",
      " |          keyword arguments for specific components.\n",
      " |      RETURNS (Doc): A container for accessing the annotations.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#call\n",
      " |  \n",
      " |  __init__(self, vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |      Initialise a Language object.\n",
      " |      \n",
      " |      vocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\n",
      " |      meta (dict): Custom meta data for the Language class. Is written to by\n",
      " |          models to add model meta data.\n",
      " |      max_length (int): Maximum number of characters in a single text. The\n",
      " |          current models may run out memory on extremely long texts, due to\n",
      " |          large internal allocations. You should segment these texts into\n",
      " |          meaningful units, e.g. paragraphs, subsections etc, before passing\n",
      " |          them to spaCy. Default maximum length is 1,000,000 charas (1mb). As\n",
      " |          a rule of thumb, if all pipeline components are enabled, spaCy's\n",
      " |          default models currently requires roughly 1GB of temporary memory per\n",
      " |          100,000 characters in one text.\n",
      " |      create_tokenizer (Callable): Function that takes the nlp object and\n",
      " |          returns a tokenizer.\n",
      " |      batch_size (int): Default batch size for pipe and evaluate.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#init\n",
      " |  \n",
      " |  add_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, before: Union[str, int, NoneType] = None, after: Union[str, int, NoneType] = None, first: Union[bool, NoneType] = None, last: Union[bool, NoneType] = None, source: Union[ForwardRef('Language'), NoneType] = None, config: Dict[str, Any] = {}, raw_config: Union[thinc.config.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Add a component to the processing pipeline. Valid components are\n",
      " |      callables that take a `Doc` object, modify it and return it. Only one\n",
      " |      of before/after/first/last can be set. Default behaviour is \"last\".\n",
      " |      \n",
      " |      factory_name (str): Name of the component factory.\n",
      " |      name (str): Name of pipeline component. Overwrites existing\n",
      " |          component.name attribute if available. If no name is set and\n",
      " |          the component exposes no name attribute, component.__name__ is\n",
      " |          used. An error is raised if a name already exists in the pipeline.\n",
      " |      before (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly before.\n",
      " |      after (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly after.\n",
      " |      first (bool): If True, insert component first in the pipeline.\n",
      " |      last (bool): If True, insert component last in the pipeline.\n",
      " |      source (Language): Optional loaded nlp object to copy the pipeline\n",
      " |          component from.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#add_pipe\n",
      " |  \n",
      " |  analyze_pipes(self, *, keys: List[str] = ['assigns', 'requires', 'scores', 'retokenizes'], pretty: bool = False) -> Union[Dict[str, Any], NoneType]\n",
      " |      Analyze the current pipeline components, print a summary of what\n",
      " |      they assign or require and check that all requirements are met.\n",
      " |      \n",
      " |      keys (List[str]): The meta values to display in the table. Corresponds\n",
      " |          to values in FactoryMeta, defined by @Language.factory decorator.\n",
      " |      pretty (bool): Pretty-print the results.\n",
      " |      RETURNS (dict): The data.\n",
      " |  \n",
      " |  begin_training(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Create an optimizer, usually using the [training.optimizer] config.\n",
      " |  \n",
      " |  create_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, config: Dict[str, Any] = {}, raw_config: Union[thinc.config.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Create a pipeline component. Mostly used internally. To create and\n",
      " |      add a component to the pipeline, you can use nlp.add_pipe.\n",
      " |      \n",
      " |      factory_name (str): Name of component factory.\n",
      " |      name (Optional[str]): Optional name to assign to component instance.\n",
      " |          Defaults to factory name if not set.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#create_pipe\n",
      " |  \n",
      " |  create_pipe_from_source(self, source_name: str, source: 'Language', *, name: str) -> Tuple[ForwardRef('Pipe'), str]\n",
      " |      Create a pipeline component by copying it from an existing model.\n",
      " |      \n",
      " |      source_name (str): Name of the component in the source pipeline.\n",
      " |      source (Language): The source nlp object to copy from.\n",
      " |      name (str): Optional alternative name to use in current pipeline.\n",
      " |      RETURNS (Tuple[Callable, str]): The component and its factory name.\n",
      " |  \n",
      " |  disable_pipe(self, name: str) -> None\n",
      " |      Disable a pipeline component. The component will still exist on\n",
      " |      the nlp object, but it won't be run as part of the pipeline. Does\n",
      " |      nothing if the component is already disabled.\n",
      " |      \n",
      " |      name (str): The name of the component to disable.\n",
      " |  \n",
      " |  disable_pipes(self, *names) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      This method has been deprecated since 3.0\n",
      " |  \n",
      " |  enable_pipe(self, name: str) -> None\n",
      " |      Enable a previously disabled pipeline component so it's run as part\n",
      " |      of the pipeline. Does nothing if the component is already enabled.\n",
      " |      \n",
      " |      name (str): The name of the component to enable.\n",
      " |  \n",
      " |  evaluate(self, examples: Iterable[spacy.training.example.Example], *, batch_size: Union[int, NoneType] = None, scorer: Union[spacy.scorer.Scorer, NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, scorer_cfg: Union[Dict[str, Any], NoneType] = None) -> Dict[str, Any]\n",
      " |      Evaluate a model's pipeline components.\n",
      " |      \n",
      " |      examples (Iterable[Example]): `Example` objects.\n",
      " |      batch_size (Optional[int]): Batch size to use.\n",
      " |      scorer (Optional[Scorer]): Scorer to use. If not passed in, a new one\n",
      " |          will be created.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      scorer_cfg (dict): An optional dictionary with extra keyword arguments\n",
      " |          for the scorer.\n",
      " |      \n",
      " |      RETURNS (Scorer): The scorer containing the evaluation results.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#evaluate\n",
      " |  \n",
      " |  from_bytes(self, bytes_data: bytes, *, exclude: Iterable[str] = []) -> 'Language'\n",
      " |      Load state from a binary string.\n",
      " |      \n",
      " |      bytes_data (bytes): The data to load from.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_bytes\n",
      " |  \n",
      " |  from_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = [], overrides: Dict[str, Any] = {}) -> 'Language'\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it. If the saved `Language` object contains a model, the\n",
      " |      model will be loaded.\n",
      " |      \n",
      " |      path (str / Path): A path to a directory.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The modified `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_disk\n",
      " |  \n",
      " |  get_pipe(self, name: str) -> 'Pipe'\n",
      " |      Get a pipeline component for a given component name.\n",
      " |      \n",
      " |      name (str): Name of pipeline component to get.\n",
      " |      RETURNS (callable): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#get_pipe\n",
      " |  \n",
      " |  get_pipe_config(self, name: str) -> thinc.config.Config\n",
      " |      Get the config used to create a pipeline component.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (Config): The config used to create the pipeline component.\n",
      " |  \n",
      " |  get_pipe_meta(self, name: str) -> 'FactoryMeta'\n",
      " |      Get the meta information for a given component name.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given component name.\n",
      " |  \n",
      " |  has_pipe(self, name: str) -> bool\n",
      " |      Check if a component name is present in the pipeline. Equivalent to\n",
      " |      `name in nlp.pipe_names`.\n",
      " |      \n",
      " |      name (str): Name of the component.\n",
      " |      RETURNS (bool): Whether a component of the name exists in the pipeline.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#has_pipe\n",
      " |  \n",
      " |  initialize(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Initialize the pipe for training, using data examples if available.\n",
      " |      \n",
      " |      get_examples (Callable[[], Iterable[Example]]): Optional function that\n",
      " |          returns gold-standard Example objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer to use for updates. If not\n",
      " |          provided, will be created using the .create_optimizer() method.\n",
      " |      RETURNS (thinc.api.Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#initialize\n",
      " |  \n",
      " |  make_doc(self, text: str) -> spacy.tokens.doc.Doc\n",
      " |      Turn a text into a Doc object.\n",
      " |      \n",
      " |      text (str): The text to process.\n",
      " |      RETURNS (Doc): The processed doc.\n",
      " |  \n",
      " |  pipe(self, texts: Union[Iterable[Union[str, spacy.tokens.doc.Doc]], Iterable[Tuple[Union[str, spacy.tokens.doc.Doc], ~_AnyContext]]], *, as_tuples: bool = False, batch_size: Union[int, NoneType] = None, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, n_process: int = 1) -> Union[Iterator[spacy.tokens.doc.Doc], Iterator[Tuple[spacy.tokens.doc.Doc, ~_AnyContext]]]\n",
      " |      Process texts as a stream, and yield `Doc` objects in order.\n",
      " |      \n",
      " |      texts (Iterable[Union[str, Doc]]): A sequence of texts or docs to\n",
      " |          process.\n",
      " |      as_tuples (bool): If set to True, inputs should be a sequence of\n",
      " |          (text, context) tuples. Output will then be a sequence of\n",
      " |          (doc, context) tuples. Defaults to False.\n",
      " |      batch_size (Optional[int]): The number of texts to buffer.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
      " |      YIELDS (Doc): Documents in the order of the original text.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#pipe\n",
      " |  \n",
      " |  rehearse(self, examples: Iterable[spacy.training.example.Example], *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = []) -> Dict[str, float]\n",
      " |      Make a \"rehearsal\" update to the models in the pipeline, to prevent\n",
      " |      forgetting. Rehearsal updates run an initial copy of the model over some\n",
      " |      data, and update the model so its current predictions are more like the\n",
      " |      initial ones. This is useful for keeping a pretrained model on-track,\n",
      " |      even if you're updating it with a smaller set of examples.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of `Example` objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      RETURNS (dict): Results from the update.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> raw_text_batches = minibatch(raw_texts)\n",
      " |          >>> for labelled_batch in minibatch(examples):\n",
      " |          >>>     nlp.update(labelled_batch)\n",
      " |          >>>     raw_batch = [Example.from_dict(nlp.make_doc(text), {}) for text in next(raw_text_batches)]\n",
      " |          >>>     nlp.rehearse(raw_batch)\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rehearse\n",
      " |  \n",
      " |  remove_pipe(self, name: str) -> Tuple[str, ForwardRef('Pipe')]\n",
      " |      Remove a component from the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to remove.\n",
      " |      RETURNS (tuple): A `(name, component)` tuple of the removed component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#remove_pipe\n",
      " |  \n",
      " |  rename_pipe(self, old_name: str, new_name: str) -> None\n",
      " |      Rename a pipeline component.\n",
      " |      \n",
      " |      old_name (str): Name of the component to rename.\n",
      " |      new_name (str): New name of the component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rename_pipe\n",
      " |  \n",
      " |  replace_listeners(self, tok2vec_name: str, pipe_name: str, listeners: Iterable[str]) -> None\n",
      " |      Find listener layers (connecting to a token-to-vector embedding\n",
      " |      component) of a given pipeline component model and replace\n",
      " |      them with a standalone copy of the token-to-vector layer. This can be\n",
      " |      useful when training a pipeline with components sourced from an existing\n",
      " |      pipeline: if multiple components (e.g. tagger, parser, NER) listen to\n",
      " |      the same tok2vec component, but some of them are frozen and not updated,\n",
      " |      their performance may degrade significally as the tok2vec component is\n",
      " |      updated with new data. To prevent this, listeners can be replaced with\n",
      " |      a standalone tok2vec layer that is owned by the component and doesn't\n",
      " |      change if the component isn't updated.\n",
      " |      \n",
      " |      tok2vec_name (str): Name of the token-to-vector component, typically\n",
      " |          \"tok2vec\" or \"transformer\".\n",
      " |      pipe_name (str): Name of pipeline component to replace listeners for.\n",
      " |      listeners (Iterable[str]): The paths to the listeners, relative to the\n",
      " |          component config, e.g. [\"model.tok2vec\"]. Typically, implementations\n",
      " |          will only connect to one tok2vec component, [model.tok2vec], but in\n",
      " |          theory, custom models can use multiple listeners. The value here can\n",
      " |          either be an empty list to not replace any listeners, or a complete\n",
      " |          (!) list of the paths to all listener layers used by the model.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_listeners\n",
      " |  \n",
      " |  replace_pipe(self, name: str, factory_name: str, *, config: Dict[str, Any] = {}, validate: bool = True) -> 'Pipe'\n",
      " |      Replace a component in the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to replace.\n",
      " |      factory_name (str): Factory name of replacement component.\n",
      " |      config (Optional[Dict[str, Any]]): Config parameters to use for this\n",
      " |          component. Will be merged with default config, if available.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The new pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_pipe\n",
      " |  \n",
      " |  resume_training(self, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Continue training a pretrained model.\n",
      " |      \n",
      " |      Create and return an optimizer, and initialize \"rehearsal\" for any pipeline\n",
      " |      component that has a .rehearse() method. Rehearsal is used to prevent\n",
      " |      models from \"forgetting\" their initialized \"knowledge\". To perform\n",
      " |      rehearsal, collect samples of text you want the models to retain performance\n",
      " |      on, and call nlp.rehearse() with a batch of Example objects.\n",
      " |      \n",
      " |      RETURNS (Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#resume_training\n",
      " |  \n",
      " |  select_pipes(self, *, disable: Union[str, Iterable[str], NoneType] = None, enable: Union[str, Iterable[str], NoneType] = None) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      disable (str or iterable): The name(s) of the pipes to disable\n",
      " |      enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#select_pipes\n",
      " |  \n",
      " |  set_error_handler(self, error_handler: Callable[[str, ForwardRef('Pipe'), List[spacy.tokens.doc.Doc], Exception], NoReturn])\n",
      " |      Set an error handler object for all the components in the pipeline that implement\n",
      " |      a set_error_handler function.\n",
      " |      \n",
      " |      error_handler (Callable[[str, Pipe, List[Doc], Exception], NoReturn]):\n",
      " |          Function that deals with a failing batch of documents. This callable function should take in\n",
      " |          the component's name, the component itself, the offending batch of documents, and the exception\n",
      " |          that was thrown.\n",
      " |      DOCS: https://spacy.io/api/language#set_error_handler\n",
      " |  \n",
      " |  to_bytes(self, *, exclude: Iterable[str] = []) -> bytes\n",
      " |      Serialize the current state to a binary string.\n",
      " |      \n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (bytes): The serialized form of the `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_bytes\n",
      " |  \n",
      " |  to_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = []) -> None\n",
      " |      Save the current state to a directory.  If a model is loaded, this\n",
      " |      will include the model.\n",
      " |      \n",
      " |      path (str / Path): Path to a directory, which will be created if\n",
      " |          it doesn't exist.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_disk\n",
      " |  \n",
      " |  update(self, examples: Iterable[spacy.training.example.Example], _: Union[Any, NoneType] = None, *, drop: float = 0.0, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = [], annotates: Iterable[str] = [])\n",
      " |      Update the models in the pipeline.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of examples\n",
      " |      _: Should not be set - serves to catch backwards-incompatible scripts.\n",
      " |      drop (float): The dropout rate.\n",
      " |      sgd (Optimizer): An optimizer.\n",
      " |      losses (Dict[str, float]): Dictionary to update with the loss, keyed by\n",
      " |          component.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      annotates (Iterable[str]): Names of components that should set\n",
      " |          annotations on the predicted examples after updating.\n",
      " |      RETURNS (Dict[str, float]): The updated losses dictionary\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#update\n",
      " |  \n",
      " |  use_params(self, params: Union[dict, NoneType])\n",
      " |      Replace weights of models in the pipeline with those provided in the\n",
      " |      params dictionary. Can be used as a contextmanager, in which case,\n",
      " |      models go back to their original weights after the block.\n",
      " |      \n",
      " |      params (dict): A dictionary of parameters keyed by model ID.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> with nlp.use_params(optimizer.averages):\n",
      " |          >>>     nlp.to_disk(\"/tmp/checkpoint\")\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#use_params\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  component(name: str, *, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, func: Union[ForwardRef('Pipe'), NoneType] = None) -> Callable from builtins.type\n",
      " |      Register a new pipeline component. Can be used for stateless function\n",
      " |      components that don't require a separate factory. Can be used as a\n",
      " |      decorator on a function or classmethod, or called as a function with the\n",
      " |      factory provided as the func keyword argument. To create a component and\n",
      " |      add it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#component\n",
      " |  \n",
      " |  factory(name: str, *, default_config: Dict[str, Any] = {}, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, default_score_weights: Dict[str, Union[float, NoneType]] = {}, func: Union[Callable, NoneType] = None) -> Callable from builtins.type\n",
      " |      Register a new pipeline component factory. Can be used as a decorator\n",
      " |      on a function or classmethod, or called as a function with the factory\n",
      " |      provided as the func keyword argument. To create a component and add\n",
      " |      it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      default_config (Dict[str, Any]): Default configuration, describing the\n",
      " |          default values of the factory arguments.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      default_score_weights (Dict[str, Optional[float]]): The scores to report during\n",
      " |          training, and their default weight towards the final score used to\n",
      " |          select the best model. Weights should sum to 1.0 per component and\n",
      " |          will be combined and normalized for the whole pipeline. If None,\n",
      " |          the score won't be shown in the logs or be weighted.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#factory\n",
      " |  \n",
      " |  from_config(config: Union[Dict[str, Any], thinc.config.Config] = {}, *, vocab: Union[spacy.vocab.Vocab, bool] = True, disable: Iterable[str] = [], exclude: Iterable[str] = [], meta: Dict[str, Any] = {}, auto_fill: bool = True, validate: bool = True) -> 'Language' from builtins.type\n",
      " |      Create the nlp object from a loaded config. Will set up the tokenizer\n",
      " |      and language data, add pipeline components etc. If no config is provided,\n",
      " |      the default config of the given language is used.\n",
      " |      \n",
      " |      config (Dict[str, Any] / Config): The loaded config.\n",
      " |      vocab (Vocab): A Vocab object. If True, a vocab is created.\n",
      " |      disable (Iterable[str]): Names of pipeline components to disable.\n",
      " |          Disabled pipes will be loaded but they won't be run unless you\n",
      " |          explicitly enable them by calling nlp.enable_pipe.\n",
      " |      exclude (Iterable[str]): Names of pipeline components to exclude.\n",
      " |          Excluded components won't be loaded.\n",
      " |      meta (Dict[str, Any]): Meta overrides for nlp.meta.\n",
      " |      auto_fill (bool): Automatically fill in missing values in config based\n",
      " |          on defaults and function argument annotations.\n",
      " |      validate (bool): Validate the component config and arguments against\n",
      " |          the types expected by the factory.\n",
      " |      RETURNS (Language): The initialized Language class.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_config\n",
      " |  \n",
      " |  get_factory_meta(name: str) -> 'FactoryMeta' from builtins.type\n",
      " |      Get the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given factory name.\n",
      " |  \n",
      " |  get_factory_name(name: str) -> str from builtins.type\n",
      " |      Get the internal factory name based on the language subclass.\n",
      " |      \n",
      " |      name (str): The factory name.\n",
      " |      RETURNS (str): The internal factory name.\n",
      " |  \n",
      " |  has_factory(name: str) -> bool from builtins.type\n",
      " |      RETURNS (bool): Whether a factory of that name is registered.\n",
      " |  \n",
      " |  set_factory_meta(name: str, value: 'FactoryMeta') -> None from builtins.type\n",
      " |      Set the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      value (FactoryMeta): The meta to set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  component_names\n",
      " |      Get the names of the available pipeline components. Includes all\n",
      " |      active and inactive pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  components\n",
      " |      Get all (name, component) tuples in the pipeline, including the\n",
      " |      currently disabled components.\n",
      " |  \n",
      " |  config\n",
      " |      Trainable config for the current language instance. Includes the\n",
      " |      current pipeline components, as well as default training config.\n",
      " |      \n",
      " |      RETURNS (thinc.api.Config): The config.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#config\n",
      " |  \n",
      " |  disabled\n",
      " |      Get the names of all disabled components.\n",
      " |      \n",
      " |      RETURNS (List[str]): The disabled components.\n",
      " |  \n",
      " |  factory_names\n",
      " |      Get names of all available factories.\n",
      " |      \n",
      " |      RETURNS (List[str]): The factory names.\n",
      " |  \n",
      " |  meta\n",
      " |      Custom meta data of the language class. If a model is loaded, this\n",
      " |      includes details from the model's meta.json.\n",
      " |      \n",
      " |      RETURNS (Dict[str, Any]): The meta.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#meta\n",
      " |  \n",
      " |  path\n",
      " |  \n",
      " |  pipe_factories\n",
      " |      Get the component factories for the available pipeline components.\n",
      " |      \n",
      " |      RETURNS (Dict[str, str]): Factory names, keyed by component names.\n",
      " |  \n",
      " |  pipe_labels\n",
      " |      Get the labels set by the pipeline components, if available (if\n",
      " |      the component exposes a labels property).\n",
      " |      \n",
      " |      RETURNS (Dict[str, List[str]]): Labels keyed by component name.\n",
      " |  \n",
      " |  pipe_names\n",
      " |      Get names of available active pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  pipeline\n",
      " |      The processing pipeline consisting of (name, component) tuples. The\n",
      " |      components are called on the Doc in order as it passes through the\n",
      " |      pipeline.\n",
      " |      \n",
      " |      RETURNS (List[Tuple[str, Pipe]]): The pipeline.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __annotations__ = {'_factory_meta': typing.Dict[str, ForwardRef('Facto...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caring-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple is looking for buying. a U.K. startup for $1 billion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "younger-machinery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple is looking for buying. a U.K. startup for $1 billion"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collect-belly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Apple\n",
      "1 is\n",
      "2 looking\n",
      "3 for\n",
      "4 buying\n",
      "5 .\n",
      "6 a\n",
      "7 U.K.\n",
      "8 startup\n",
      "9 for\n",
      "10 $\n",
      "11 1\n",
      "12 billion\n"
     ]
    }
   ],
   "source": [
    "# '$' ve '.' işaretine dikkat\n",
    "\n",
    "for i,token in enumerate(doc):\n",
    "    print(i, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-spirit",
   "metadata": {},
   "source": [
    "## Parts of Speech [POS] Tagging (Kelime Türü Etiketleme)\n",
    "\n",
    "\n",
    "- Alphabetical listing\n",
    "\n",
    "- ADJ: adjective\n",
    "- ADP: adposition\n",
    "- ADV: adverb\n",
    "- AUX: auxiliary\n",
    "- CCONJ: coordinating conjunction\n",
    "- DET: determiner\n",
    "- INTJ: interjection\n",
    "- NOUN: noun\n",
    "- NUM: numeral\n",
    "- PART: particle\n",
    "- PRON: pronoun\n",
    "- PROPN: proper noun\n",
    "- PUNCT: punctuation\n",
    "- SCONJ: subordinating conjunction\n",
    "- SYM: symbol\n",
    "- VERB: verb\n",
    "- X: other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "altered-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS(pos):\n",
    "    \n",
    "    \"\"\"\n",
    "    How to Using:\n",
    "    \n",
    "    for token in doc:\n",
    "\n",
    "        print(POS(token.pos_))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import spacy\n",
    "    \n",
    "    pos_dicts =  \\\n",
    "        {\"ADJ\": \"sıfat\",\n",
    "        \"ADP\": \"adpozisyon\",\n",
    "        \"ADV\": \"zarf\",\n",
    "        \"AUX\": \"yardımcı\",\n",
    "        \"CCONJ\": \"koordine edici bağlantı\",\n",
    "        \"DET\": \"belirleyici\",\n",
    "        \"INTJ\": \"ünlem\",\n",
    "        \"NOUN\": \"isim\",\n",
    "        \"NUM\": \"sayı\",\n",
    "        \"PART\": \"parçacık\",\n",
    "        \"PRON\": \"zamir\",\n",
    "        \"PROPN\": \"özel isim\",\n",
    "        \"PUNCT\": \"noktalama\",\n",
    "        \"SCONJ\": \"bağımlı bağlaç\",\n",
    "        \"SYM\": \"sembol\",\n",
    "        \"VERB\": \"fiil\",\n",
    "        \"X\": \"diğer\"}\n",
    "    \n",
    "    \n",
    "    return pos_dicts[pos].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "executive-review",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text : Apple          , POS : (PROPN  , ÖZEL ISIM)\n",
      "Text : is             , POS : (AUX    , YARDIMCI)\n",
      "Text : looking        , POS : (VERB   , FIIL)\n",
      "Text : for            , POS : (ADP    , ADPOZISYON)\n",
      "Text : buying         , POS : (NOUN   , ISIM)\n",
      "Text : .              , POS : (PUNCT  , NOKTALAMA)\n",
      "Text : a              , POS : (DET    , BELIRLEYICI)\n",
      "Text : U.K.           , POS : (PROPN  , ÖZEL ISIM)\n",
      "Text : startup        , POS : (VERB   , FIIL)\n",
      "Text : for            , POS : (ADP    , ADPOZISYON)\n",
      "Text : $              , POS : (SYM    , SEMBOL)\n",
      "Text : 1              , POS : (NUM    , SAYI)\n",
      "Text : billion        , POS : (NUM    , SAYI)\n"
     ]
    }
   ],
   "source": [
    "# isim, fiil, sıfat, bağlaç vb.\n",
    "\n",
    "for token in doc:\n",
    "    \n",
    "    print(f\"Text : {token.text:{15}}, POS : ({token.pos_:{7}}, {POS(token.pos_)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-manchester",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "funny-milton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple is looking for buying. a U.K. startup for $1 billion"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "running-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "regulation-pioneer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking for buying. a \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dirty-bonus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7818795614b547df9e5d84edd9ba1887-0\" class=\"displacy\" width=\"1250\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: purple; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">buying.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7818795614b547df9e5d84edd9ba1887-0-0\" stroke-width=\"2px\" d=\"M70,152.0 C70,52.0 245.0,52.0 245.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7818795614b547df9e5d84edd9ba1887-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,154.0 L62,142.0 78,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7818795614b547df9e5d84edd9ba1887-0-1\" stroke-width=\"2px\" d=\"M170,152.0 C170,102.0 240.0,102.0 240.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7818795614b547df9e5d84edd9ba1887-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M170,154.0 L162,142.0 178,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7818795614b547df9e5d84edd9ba1887-0-2\" stroke-width=\"2px\" d=\"M270,152.0 C270,102.0 340.0,102.0 340.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7818795614b547df9e5d84edd9ba1887-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340.0,154.0 L348.0,142.0 332.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7818795614b547df9e5d84edd9ba1887-0-3\" stroke-width=\"2px\" d=\"M370,152.0 C370,102.0 440.0,102.0 440.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7818795614b547df9e5d84edd9ba1887-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M440.0,154.0 L448.0,142.0 432.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7818795614b547df9e5d84edd9ba1887-0-4\" stroke-width=\"2px\" d=\"M570,152.0 C570,102.0 640.0,102.0 640.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7818795614b547df9e5d84edd9ba1887-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570,154.0 L562,142.0 578,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7818795614b547df9e5d84edd9ba1887-0-5\" stroke-width=\"2px\" d=\"M670,152.0 C670,102.0 740.0,102.0 740.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7818795614b547df9e5d84edd9ba1887-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,154.0 L662,142.0 678,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7818795614b547df9e5d84edd9ba1887-0-6\" stroke-width=\"2px\" d=\"M770,152.0 C770,102.0 840.0,102.0 840.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7818795614b547df9e5d84edd9ba1887-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M840.0,154.0 L848.0,142.0 832.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7818795614b547df9e5d84edd9ba1887-0-7\" stroke-width=\"2px\" d=\"M970,152.0 C970,52.0 1145.0,52.0 1145.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7818795614b547df9e5d84edd9ba1887-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M970,154.0 L962,142.0 978,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7818795614b547df9e5d84edd9ba1887-0-8\" stroke-width=\"2px\" d=\"M1070,152.0 C1070,102.0 1140.0,102.0 1140.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7818795614b547df9e5d84edd9ba1887-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1070,154.0 L1062,142.0 1078,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7818795614b547df9e5d84edd9ba1887-0-9\" stroke-width=\"2px\" d=\"M870,152.0 C870,2.0 1150.0,2.0 1150.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7818795614b547df9e5d84edd9ba1887-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1150.0,154.0 L1158.0,142.0 1142.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = \"dep\",options = {\"distance\" : 100, \"color\" : \"purple\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-performer",
   "metadata": {},
   "source": [
    "## Named Entity Recognition NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "correct-heart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple is looking for buying. a U.K. startup for $1 billion"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efficient-debut",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Apple, U.K., $1 billion)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "roman-context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text : Apple          , NER : ORG\n",
      "Text : U.K.           , NER : GPE\n",
      "Text : $1 billion     , NER : MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "\n",
    "    print(f\"Text : {ent.text:{15}}, NER : {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aggressive-guide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple is looking for buying a UK startup for $1 billion in 2020"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(\"Apple is looking for buying a UK startup for $1 billion in 2020\")\n",
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "imported-suffering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking for buying a \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    UK\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2020\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc2,style = \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-frost",
   "metadata": {},
   "source": [
    "## Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adjusted-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"Apple is looking for buying a UK startup. Government has given permission\"\n",
    "text_2 = \"Apple is looking for buying a U.K. startup. Government has given permission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "chief-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dedicated-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "collaborative-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = nlp(text_1)\n",
    "doc_2 = nlp(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "european-universal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking for buying a UK startup.\n",
      "Government has given permission\n"
     ]
    }
   ],
   "source": [
    "for sent in doc_1.sents:\n",
    "    \n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "subjective-horse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking for buying a U.K. startup.\n",
      "Government has given permission\n"
     ]
    }
   ],
   "source": [
    "# Noktalarda takılmadan ayırabildi.\n",
    "\n",
    "for sent in doc_2.sents:\n",
    "\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-huntington",
   "metadata": {},
   "source": [
    "## Phrase Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adjustable-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "square-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "reverse-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world! hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "measured-employer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello, world! hello world"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_3 = nlp(text)\n",
    "doc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "municipal-hobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "for i in doc_3.sents:\n",
    "    print(i.text,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "numerous-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in doc_3.ents:\n",
    "    print(i.text,i.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "invisible-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [[ {\"LOWER\" : \"hello\"}, {\"IS_PUNCT\" : True}, {\"LOWER\" : \"world\"} ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "patient-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add(\"hw\",pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "hindu-population",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17790654416186116455, 0, 3)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MATCH_ID , START , STOP\n",
    "\n",
    "matches = matcher(doc_3)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "stable-surrey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match Id : 17790654416186116455\n",
      "String Id : hw\n",
      "Start : 0\n",
      "Stop : 3\n",
      "Span : Hello, world\n"
     ]
    }
   ],
   "source": [
    "for match_id ,start, stop in matches:\n",
    "    \n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc_3[start:stop].text\n",
    "    print(\"Match Id : {}\\nString Id : {}\\nStart : {}\\nStop : {}\\nSpan : {}\".format(match_id,\n",
    "                                                                                   string_id,\n",
    "                                                                                   start,\n",
    "                                                                                   stop,\n",
    "                                                                                   span))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-compromise",
   "metadata": {},
   "source": [
    "## Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "received-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"my phone number is 123. ohh its wrong one. correct one is 1234567890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "existing-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "selective-settlement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(19, 20), match='1'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"\\d\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "experienced-running",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(19, 22), match='123'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"\\d+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "alpine-torture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(58, 68), match='1234567890'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"\\d{10}\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "arctic-virtue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(19, 22), match='123'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"\\d{3}\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "likely-hammer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n",
      "phone\n",
      "number\n",
      "is\n",
      "123\n",
      "ohh\n",
      "its\n",
      "wrong\n",
      "one\n",
      "correct\n",
      "one\n",
      "is\n",
      "1234567890\n"
     ]
    }
   ],
   "source": [
    "for i in re.findall(r\"\\w+\",text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "final-clark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['phon'], ['phone'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"ph..\",text),re.findall(r\"ph...\",text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "extended-nerve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123', '1234567890']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[\\d]+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "nervous-installation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123', '1234567890']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[\\d]{3,10}\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "vanilla-skating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123', '1234567890']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\d{3,10}\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "smoking-ceramic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '3', '1', '34']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[143]+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adapted-asian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['my',\n",
       "  'phone',\n",
       "  'number',\n",
       "  'is',\n",
       "  'ohh',\n",
       "  'its',\n",
       "  'wrong',\n",
       "  'one',\n",
       "  'correct',\n",
       "  'one',\n",
       "  'is'],\n",
       " ['123', '1234567890'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[A-za-z]+\",text),re.findall(r\"[^A-za-z. ]+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "proof-trash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['correct']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"c......\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "robust-challenge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', '.', 'c', 'c']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[c.....]+\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-eclipse",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "unlimited-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "chief-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"net income was $9.4 million compared to the prior year of 2.7$ million\",\n",
    "         \"revenue exceeds twelve billion dollars with a loss of $1b.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "gross-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "framed-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text ---> nlp pipe [tokenizer, tagger , parser , ner , ....] ---> Doc\n",
    "\n",
    "docs = nlp.pipe(texts, disable = [\"tagger\",\"parser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "tired-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text : $9.4 million             , ENT : MONEY\n",
      "Text : the prior year           , ENT : DATE\n",
      "Text : 2.7$ million             , ENT : MONEY\n",
      "\n",
      "Text : twelve billion dollars   , ENT : MONEY\n",
      "Text : 1b                       , ENT : MONEY\n",
      "\n",
      "CPU times: user 6.81 ms, sys: 717 µs, total: 7.53 ms\n",
      "Wall time: 7.08 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "docs = nlp.pipe(texts, disable = [\"tagger\",\"parser\"])\n",
    "\n",
    "for doc in docs:\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        print(f\"Text : {ent.text:{25}}, ENT : {ent.label_}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "noticed-croatia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text : $9.4 million             , Ent : MONEY\n",
      "Text : the prior year           , Ent : DATE\n",
      "Text : 2.7$ million             , Ent : MONEY\n",
      "\n",
      "Text : twelve billion dollars   , Ent : MONEY\n",
      "Text : 1b                       , Ent : MONEY\n",
      "\n",
      "CPU times: user 8.35 ms, sys: 1.21 ms, total: 9.56 ms\n",
      "Wall time: 8.77 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "docs = nlp.pipe(texts)\n",
    "\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        print(f\"Text : {ent.text:{25}}, Ent : {ent.label_}\")\n",
    "        \n",
    "    print()\n",
    "    \n",
    "# Süreleri karşılaştır Biz bazı süreçleri yukarda görmezden geldik !!! Süre burda Yükseldi !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-contribution",
   "metadata": {},
   "source": [
    "## Hashtags and Emoji Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "lovely-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "psychological-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "south-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emoji = [\"😀\", \"😃\", \"😂\", \"🤣\", \"😊\", \"😍\"]  # Positive emoji\n",
    "neg_emoji = [\"😞\", \"😠\", \"😩\", \"😢\", \"😭\", \"😒\"]  # Negative emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "looking-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [[{\"ORTH\" : emoji}] for emoji in pos_emoji]\n",
    "neg = [[{\"ORTH\" : emoji}] for emoji in neg_emoji]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "animal-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = [[ {\"TEXT\" : \"#\"}, {\"IS_ASCII\" : True} ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "balanced-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = Matcher(nlp.vocab)\n",
    "\n",
    "matches.add(\"hashtags\",hashtags)\n",
    "matches.add(\"pos\",pos)\n",
    "matches.add(\"neg\",neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "described-moderator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello guys 😀😂 😞 #kgptalkie"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Hello guys 😀😂 😞 #kgptalkie\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "middle-delay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12715761412317525803, 2, 3),\n",
       " (12715761412317525803, 3, 4),\n",
       " (425, 4, 5),\n",
       " (12116372008979164942, 5, 7)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "surrounded-investigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match Id : 12715761412317525803\n",
      "String Id : pos\n",
      "Start : 2\n",
      "Stop : 3\n",
      "Span : 😀\n",
      "\n",
      "Match Id : 12715761412317525803\n",
      "String Id : pos\n",
      "Start : 3\n",
      "Stop : 4\n",
      "Span : 😂\n",
      "\n",
      "Match Id : 425\n",
      "String Id : neg\n",
      "Start : 4\n",
      "Stop : 5\n",
      "Span : 😞\n",
      "\n",
      "Match Id : 12116372008979164942\n",
      "String Id : hashtags\n",
      "Start : 5\n",
      "Stop : 7\n",
      "Span : #kgptalkie\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match_id,start,stop in matches(doc):\n",
    "    \n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:stop]\n",
    "    \n",
    "    print(\"Match Id : {}\\nString Id : {}\\nStart : {}\\nStop : {}\\nSpan : {}\".format(match_id,\n",
    "                                                                                   string_id,\n",
    "                                                                                   start,\n",
    "                                                                                   stop,\n",
    "                                                                                   span))\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "desirable-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world! hello world\"\n",
    "\n",
    "doc_text = nlp(text)\n",
    "\n",
    "\n",
    "# Var yada yok 'is_punct' için\n",
    "\n",
    "pattern = [[{'LOWER': 'hello'}, {'IS_PUNCT': True, 'OP': '?'}, {'LOWER': 'world'}]]\n",
    "\n",
    "matches = Matcher(nlp.vocab)\n",
    "\n",
    "matches.add(\"hw!\",pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "supported-royal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match Id : 15881058849434767646\n",
      "String Id : hw!\n",
      "Start : 0\n",
      "Stop : 3\n",
      "Span : Hello, world\n",
      "\n",
      "Match Id : 15881058849434767646\n",
      "String Id : hw!\n",
      "Start : 4\n",
      "Stop : 6\n",
      "Span : hello world\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match_id,start,stop in matches(doc_text):\n",
    "    \n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc_text[start:stop]\n",
    "    \n",
    "    print(\"Match Id : {}\\nString Id : {}\\nStart : {}\\nStop : {}\\nSpan : {}\".format(match_id,\n",
    "                                                                                   string_id,\n",
    "                                                                                   start,\n",
    "                                                                                   stop,\n",
    "                                                                                   span))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-roman",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
